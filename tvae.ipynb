{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run create_dataset.ipynb first. Ensure you have the train_df saved in {train_df_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select training data that are of label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_dataset import separate_labels\n",
    "\n",
    "df_label_0, df_label_1 = separate_labels(file_path=) # Enter the train_df_path\n",
    "# Remember we should generate label 1 data\n",
    "# Make sure that df_label_0, df_label_1 are consistent\n",
    "real_data = df_label_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train TVAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.metadata import SingleTableMetadata\n",
    "\n",
    "metadata = SingleTableMetadata(df_label_1)\n",
    "metadata.detect_from_dataframe(df_label_1) # TODO: Fix\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.single_table import TVAESynthesizer\n",
    "synthesizer = TVAESynthesizer(metadata, verbose = True)\n",
    "synthesizer.fit(df_label_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesizer.save('tvae_synthesizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If have the synthesizer saved just load it\n",
    "# import joblib\n",
    "# synthesizer = joblib.load(\"tvae_synthesizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many synthetic data we need?\n",
    "num_generated_data_needed = df_label_0.shape[0] - df_label_1.shape[0]\n",
    "synthetic_data = synthesizer.sample(num_generated_data_needed)\n",
    "synthetic_data.to_csv(\"{Path to save the synthetic data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_dataset import load_dataset\n",
    "# If have the synthetic data saved just load it, however if you have the synthetic data generated in the previous step you can ignore this\n",
    "synthetic_data = load_dataset(file_path=) # Enter path to the synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.fidelity import *\n",
    "# PCA\n",
    "plot_pca(real_data=real_data, synthetic_data=synthetic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ks_test\n",
    "ks_test(real_data=real_data, synthetic_data=synthetic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE Plot\n",
    "kde_plot(real_data=real_data, synthetic_data=synthetic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset + test datase\n",
    "test_df = load_dataset(file_path=) # Enter path to the test dataset\n",
    "\n",
    "# Combine synthetic data and real data for the train set\n",
    "combined_train_data = pd.concat([real_data, synthetic_data], ignore_index=True)\n",
    "X_train = combined_train_data.drop(columns=[\"label\"])\n",
    "y_train = combined_train_data[\"label\"]\n",
    "X_test = test_df.drop(columns=[\"label\"])\n",
    "y_test = test_df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.utility import *\n",
    "# Logistic Regression\n",
    "evaluate(method=\"logistic_regression\", X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)\n",
    "# Random Forest\n",
    "evaluate(method=\"random_forest\", X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)\n",
    "# Catboost\n",
    "evaluate(method=\"catboost\", X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: Privacy (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
